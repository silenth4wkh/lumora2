from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import requests
import xml.etree.ElementTree as ET
from urllib.parse import quote
from datetime import datetime
import time
import re
import json
from collections import defaultdict
import threading
import queue
import os

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None

app = Flask(__name__)
CORS(app)

# Scraper k√≥d (a te k√≥db√≥l)
KW_LANG = [
    "java","python","c#",".net","dotnet","c++","cpp","golang","go","rust","php","ruby","scala","haskell",
    "kotlin","swift","objective-c"
]
KW_FE = [
    "frontend","front-end","react","angular","vue","svelte","next.js","nuxt","typescript","javascript","web developer"
]
KW_BE = [
    "backend","back-end","node","spring","spring boot","quarkus",".net core","asp.net","laravel","symfony","django","flask","fastapi"
]
KW_MOBILE = [
    "android","ios","mobile","kotlin","swift","flutter","react native","xamarin","ionic"
]
KW_DATA = [
    "data engineer","data scientist","ml engineer","machine learning","ai engineer","big data",
    "etl","elt","dwh","data warehouse","power bi","tableau","qlik","sql","spark","hadoop","db developer","snowflake","databricks"
]
KW_DEVOPS = [
    "devops","sre","site reliability","platform engineer","cloud engineer","kubernetes","docker","terraform","ansible",
    "aws","azure","gcp","cloud architect","finops","observability"
]
KW_TEST = [
    "qa","test automation","tesztautomatiz√°l√°s","tesztm√©rn√∂k","sdet","cypress","selenium","playwright","jmeter","postman"
]
KW_EMBED = [
    "embedded","firmware","fpga","rtos","bare metal","microcontroller","stm32","esp32","embedded linux","yocto","driver developer","c developer"
]
KW_SECURITY = [
    "security engineer","application security","appsec","devsecops","penetration tester","pentest","iam","siem","soc"
]
KW_ENTERPRISE = [
    "sap","abap","erp","crm","salesforce","servicenow","mendix","outsystems","navision","business central","oracle developer","ms dynamics"
]
KW_GENERAL_HU = [
    "fejleszt≈ë","programoz√≥","szoftver","szoftverm√©rn√∂k","rendszerm√©rn√∂k","alkalmaz√°sfejleszt≈ë","alkalmaz√°s √ºzemeltet≈ë","full stack","full-stack"
]

KEYWORDS = (
    KW_LANG + KW_FE + KW_BE + KW_MOBILE + KW_DATA + KW_DEVOPS +
    KW_TEST + KW_EMBED + KW_SECURITY + KW_ENTERPRISE + KW_GENERAL_HU
)

EXCLUDE_PHRASES = [
    "fejleszt≈ë pedag√≥gus","pedag√≥gus","gy√≥gypedag√≥gus","konstrukt≈ër","technol√≥gus","gy√°rt√°s",
    "p√©nzt√°ros","elad√≥","√©rt√©kes√≠t≈ë","√ºgyf√©l","adminisztr√°tor","rakt√°r","logisztika","g√©pkezel≈ë","karbantart√≥"
]

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
BASE = "https://www.profession.hu/allasok/1,0,0,{}"

# Port√°lok defini√°l√°sa
PORTALS = {
    "profession": {
        "name": "Profession.hu",
        "description": "Magyarorsz√°g legnagyobb √°ll√°skeres≈ë port√°lja",
        "status": "Akt√≠v",
        "enabled": True
    },
    "jobline": {
        "name": "Jobline.hu",
        "description": "IT specializ√°lt √°ll√°skeres≈ë port√°l",
        "status": "Tervez√©s alatt",
        "enabled": False
    },
    "cvonline": {
        "name": "CV-Online.hu",
        "description": "N√©pszer≈± √°ll√°skeres≈ë port√°l",
        "status": "Tervez√©s alatt",
        "enabled": False
    },
    "indeed": {
        "name": "Indeed.hu",
        "description": "Nemzetk√∂zi √°ll√°skeres≈ë port√°l",
        "status": "Tervez√©s alatt",
        "enabled": False
    }
}

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Kulcssz√≥-katal√≥gus (maximalista, b≈ëv√≠thet≈ë)
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
KW_LANG = [
    "java","python","c#",".net","dotnet","c++","cpp","golang","go","rust","php","ruby","scala","haskell",
    "kotlin","swift","objective-c"
]
KW_FE = [
    "frontend","front-end","react","angular","vue","svelte","next.js","nuxt","typescript","javascript","web developer"
]
KW_BE = [
    "backend","back-end","node","spring","spring boot","quarkus",".net core","asp.net","laravel","symfony","django","flask","fastapi"
]
KW_MOBILE = [
    "android","ios","mobile","kotlin","swift","flutter","react native","xamarin","ionic"
]
KW_DATA = [
    "data engineer","data scientist","ml engineer","machine learning","ai engineer","big data",
    "etl","elt","dwh","data warehouse","power bi","tableau","qlik","sql","spark","hadoop","db developer","snowflake","databricks"
]
KW_DEVOPS = [
    "devops","sre","site reliability","platform engineer","cloud engineer","kubernetes","docker","terraform","ansible",
    "aws","azure","gcp","cloud architect","finops","observability"
]
KW_TEST = [
    "qa","test automation","tesztautomatiz√°l√°s","tesztm√©rn√∂k","sdet","cypress","selenium","playwright","jmeter","postman"
]
KW_EMBED = [
    "embedded","firmware","fpga","rtos","bare metal","microcontroller","stm32","esp32","embedded linux","yocto","driver developer","c developer"
]
KW_SECURITY = [
    "security engineer","application security","appsec","devsecops","penetration tester","pentest","iam","siem","soc"
]
KW_ENTERPRISE = [
    "sap","abap","erp","crm","salesforce","servicenow","mendix","outsystems","navision","business central","oracle developer","ms dynamics"
]
KW_GENERAL_HU = [
    "fejleszt≈ë","programoz√≥","szoftver","szoftverm√©rn√∂k","rendszerm√©rn√∂k","alkalmaz√°sfejleszt≈ë","alkalmaz√°s √ºzemeltet≈ë","full stack","full-stack"
]

# √ñsszes kulcssz√≥ egy list√°ban
ALL_KEYWORDS = (
    KW_LANG + KW_FE + KW_BE + KW_MOBILE + KW_DATA + KW_DEVOPS +
    KW_TEST + KW_EMBED + KW_SECURITY + KW_ENTERPRISE + KW_GENERAL_HU
)

# Kateg√≥ri√°k defini√°l√°sa a frontend-hez
CATEGORIES = {
    "languages": {"name": "Programoz√°si nyelvek", "keywords": KW_LANG},
    "frontend": {"name": "Frontend fejleszt√©s", "keywords": KW_FE},
    "backend": {"name": "Backend fejleszt√©s", "keywords": KW_BE},
    "mobile": {"name": "Mobil fejleszt√©s", "keywords": KW_MOBILE},
    "data": {"name": "Data & AI", "keywords": KW_DATA},
    "devops": {"name": "DevOps & Cloud", "keywords": KW_DEVOPS},
    "testing": {"name": "Tesztel√©s", "keywords": KW_TEST},
    "embedded": {"name": "Embedded", "keywords": KW_EMBED},
    "security": {"name": "Biztons√°g", "keywords": KW_SECURITY},
    "enterprise": {"name": "Enterprise", "keywords": KW_ENTERPRISE},
    "general": {"name": "√Åltal√°nos IT", "keywords": KW_GENERAL_HU}
}

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"<.*?>", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def build_feed_url(keyword: str) -> str:
    return BASE.format(quote(keyword, safe=""))

def is_probably_dev(title: str, desc: str) -> bool:
    t = (title or "").lower()
    d = (desc or "").lower()
    if any(bad in t for bad in EXCLUDE_PHRASES) or any(bad in d for bad in EXCLUDE_PHRASES):
        return False
    return True

def fetch_html_jobs(source_name: str, url: str, max_pages: int = 5):
    """HTML scraping a Profession.hu √°ll√°slist√°kr√≥l - t√∂bb oldal feldolgoz√°sa"""
    if not BeautifulSoup:
        print("BeautifulSoup nincs telep√≠tve, RSS fallback haszn√°lata")
        return fetch_rss_fallback(source_name, url)
    
    all_items = []
    
    for page in range(1, max_pages + 1):
        try:
            # Oldalsz√°moz√°s hozz√°ad√°sa
            page_url = f"{url}&page={page}" if "?" in url else f"{url}?page={page}"
            
            r = requests.get(page_url, headers=HEADERS, timeout=25)
            r.raise_for_status()
            r.encoding = "utf-8"
            
            soup = BeautifulSoup(r.text, "html.parser")
            
            # Keres√©s ul.job-cards > li elemekben
            job_cards = soup.select("ul.job-cards li")
            if not job_cards:
                # Fallback: m√°s lehets√©ges szelektorok
                job_cards = soup.select(".job-card, .job-item, .listing-item, .search-result-item")
            
            print(f"DEBUG: {source_name} - Oldal {page}: {len(job_cards)} job card")
            
            if not job_cards:
                # Ha nincs t√∂bb √°ll√°s, szak√≠tsuk meg
                break
            
            for card in job_cards:
                try:
                    # Poz√≠ci√≥ c√≠me
                    title_elem = card.select_one("h3, .job-title, .position-title, .title, a[href*='/allas/']")
                    title = clean_text(title_elem.get_text()) if title_elem else ""
                    
                    # Link
                    link_elem = card.select_one("a[href*='/allas/']")
                    link = link_elem.get("href") if link_elem else ""
                    if link and not link.startswith("http"):
                        link = "https://www.profession.hu" + link
                    
                    # C√©g neve
                    company_elem = card.select_one(".company, .employer, .company-name, .job-company")
                    company = clean_text(company_elem.get_text()) if company_elem else ""
                    
                    # Lok√°ci√≥
                    location_elem = card.select_one(".location, .job-location, .city, .place")
                    location = clean_text(location_elem.get_text()) if location_elem else ""
                    
                    # Le√≠r√°s
                    desc_elem = card.select_one(".description, .job-description, .summary, .excerpt")
                    desc = clean_text(desc_elem.get_text()) if desc_elem else ""
                    
                    # D√°tum
                    date_elem = card.select_one(".date, .published, .job-date, .time")
                    pub_date = clean_text(date_elem.get_text()) if date_elem else ""
                    
                    if title and link:
                        all_items.append({
                            "Forr√°s": source_name, 
                            "Poz√≠ci√≥": title, 
                            "Link": link, 
                            "Le√≠r√°s": desc,
                            "Publik√°lva": pub_date,
                            "C√©g": company,
                            "Lok√°ci√≥": location
                        })
                        
                except Exception as e:
                    print(f"ERROR parsing job card: {e}")
                    continue
            
            # K√≠m√©let a szerver fel√©
            time.sleep(0.5)
            
        except Exception as e:
            print(f"ERROR fetching page {page}: {e}")
            break
    
    print(f"DEBUG: {source_name} - √ñsszesen {len(all_items)} √°ll√°s {max_pages} oldalr√≥l")
    return all_items

def fetch_rss_items(source_name: str, url: str):
    """RSS feed feldolgoz√°sa"""
    r = requests.get(url, headers=HEADERS, timeout=25)
    r.raise_for_status()
    r.encoding = "utf-8"
    root = ET.fromstring(r.text)
    items = []
    for it in root.findall(".//item"):
        title = clean_text(it.findtext("title",""))
        link  = (it.findtext("link","") or "").strip()
        desc  = clean_text(it.findtext("description",""))
        pub   = (it.findtext("pubDate","") or "").strip()
        items.append({"Forr√°s": source_name, "Poz√≠ci√≥": title, "Link": link, "Le√≠r√°s": desc, "Publik√°lva": pub, "C√©g": "", "Lok√°ci√≥": ""})
    return items

def fetch_rss_fallback(source_name: str, url: str):
    """RSS fallback ha HTML scraping nem m≈±k√∂dik"""
    rss_url = url + "?rss"
    r = requests.get(rss_url, headers=HEADERS, timeout=25)
    r.raise_for_status()
    r.encoding = "utf-8"
    root = ET.fromstring(r.text)
    items = []
    for it in root.findall(".//item"):
        title = clean_text(it.findtext("title",""))
        link  = (it.findtext("link","") or "").strip()
        desc  = clean_text(it.findtext("description",""))
        pub   = (it.findtext("pubDate","") or "").strip()
        items.append({"Forr√°s": source_name, "Poz√≠ci√≥": title, "Link": link, "Le√≠r√°s": desc, "Publik√°lva": pub, "C√©g": "", "Lok√°ci√≥": ""})
    return items

def _json_loads_safe(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None

def extract_company_location_from_html(html: str):
    company = None
    location = None

    if BeautifulSoup:
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup.select("script[type='application/ld+json']"):
            j = _json_loads_safe(tag.string or "")
            if not j:
                continue
            objs = j if isinstance(j, list) else [j]
            for obj in objs:
                if not isinstance(obj, dict):
                    continue
                if obj.get("@type") == "JobPosting":
                    org = obj.get("hiringOrganization") or {}
                    if isinstance(org, dict) and (org.get("name")):
                        company = company or org.get("name")
                    jl = obj.get("jobLocation")
                    jl_list = jl if isinstance(jl, list) else [jl] if isinstance(jl, dict) else []
                    for loc in jl_list:
                        addr = (loc or {}).get("address") or {}
                        parts = [addr.get("addressLocality"), addr.get("addressRegion")]
                        loc_str = ", ".join([p for p in parts if p])
                        if loc_str:
                            location = location or loc_str

    if not (company and location):
        m_items = re.search(r'"items"\s*:\s*(\[[^\]]+\])', html, re.IGNORECASE | re.DOTALL)
        if m_items:
            raw = m_items.group(1)
            raw_json = raw.replace("'", '"')
            arr = _json_loads_safe(raw_json)
            if isinstance(arr, list):
                for it in arr:
                    if not isinstance(it, dict):
                        continue
                    company = company or it.get("affiliation") or it.get("brand") or it.get("seller") or it.get("company")
                    location = location or it.get("location") or it.get("city") or it.get("region")

    if not company:
        m = re.search(r"Hirdet≈ë\s*c√©g\s*:\s*([^,<>\n]+)", html, flags=re.IGNORECASE)
        if m:
            company = m.group(1).strip()

    if not location:
        mloc = re.search(r"(Budapest(?:\s*[IVXLC]+\.?\s*ker√ºlet)?|Debrecen|Szeged|Gy≈ër|P√©cs|Miskolc|Kecskem√©t|Sz√©kesfeh√©rv√°r|Ny√≠regyh√°za|Eger|Veszpr√©m|Szombathely|Sopron|Tatab√°nya|P√°pa|√ârd|Buda√∂rs|Pest megye|B√°cs-Kiskun megye|Fej√©r megye|Gy≈ër-Moson-Sopron megye)", html, flags=re.IGNORECASE)
        if mloc:
            location = mloc.group(1).strip()

    if isinstance(location, str):
        location = location.replace("_", " ").replace("megye", "megye").strip(", ")

    return company, location

def fetch_job_meta(url: str, session: requests.Session = None, retries: int = 2, pause: float = 0.25):
    sess = session or requests.Session()
    last_err = None
    for _ in range(retries + 1):
        try:
            r = sess.get(url, headers=HEADERS, timeout=25)
            r.raise_for_status()
            r.encoding = "utf-8"
            company, location = extract_company_location_from_html(r.text)
            return (company, location)
        except Exception as e:
            last_err = e
            time.sleep(pause)
    return (None, None)

def _json_loads_safe(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None

def extract_company_location_from_html(html: str):
    """
    Pr√≥b√°lkoz√°si sorrend:
      1) JSON-LD (JobPosting)
      2) dataLayer (items[].affiliation / items[].location)
      3) DOM sz√∂veg-fallback (‚ÄûHirdet≈ë c√©g: ‚Ä¶", tipikus location elemek)
    """
    company = None
    location = None

    # 1) JSON-LD
    if BeautifulSoup:
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup.select("script[type='application/ld+json']"):
            j = _json_loads_safe(tag.string or "")
            if not j:
                continue
            objs = j if isinstance(j, list) else [j]
            for obj in objs:
                if not isinstance(obj, dict):
                    continue
                if obj.get("@type") == "JobPosting":
                    org = obj.get("hiringOrganization") or {}
                    if isinstance(org, dict) and (org.get("name")):
                        company = company or org.get("name")
                    jl = obj.get("jobLocation")
                    jl_list = jl if isinstance(jl, list) else [jl] if isinstance(jl, dict) else []
                    for loc in jl_list:
                        addr = (loc or {}).get("address") or {}
                        parts = [addr.get("addressLocality"), addr.get("addressRegion")]
                        loc_str = ", ".join([p for p in parts if p])
                        if loc_str:
                            location = location or loc_str

    # 2) dataLayer (gyors/egyszer≈± regexekkel)
    if not (company and location):
        # pr√≥b√°ljuk kifogni az items[]-et
        # pl.: "items":[{"affiliation":"4iG Nyrt.","location":"Pest_megye,_Budapest"}]
        m_items = re.search(r'"items"\s*:\s*(\[[^\]]+\])', html, re.IGNORECASE | re.DOTALL)
        if m_items:
            raw = m_items.group(1)
            # pr√≥b√°ljuk JSON kompatibiliss√© tenni (el≈ëfordulhat aposztr√≥f): durva fallback
            raw_json = raw.replace("'", '"')
            arr = _json_loads_safe(raw_json)
            if isinstance(arr, list):
                for it in arr:
                    if not isinstance(it, dict):
                        continue
                    company = company or it.get("affiliation") or it.get("brand") or it.get("seller") or it.get("company")
                    location = location or it.get("location") or it.get("city") or it.get("region")

    # 3) DOM sz√∂veg-fallbackok (regex)
    if not company:
        m = re.search(r"Hirdet≈ë\s*c√©g\s*:\s*([^,<>\n]+)", html, flags=re.IGNORECASE)
        if m:
            company = m.group(1).strip()

    if not location:
        # keress√ºnk tipikus magyar v√°ros/megye mint√°kat az oldal f≈ë tartalm√°ban
        mloc = re.search(r"(Budapest(?:\s*[IVXLC]+\.?\s*ker√ºlet)?|Debrecen|Szeged|Gy≈ër|P√©cs|Miskolc|Kecskem√©t|Sz√©kesfeh√©rv√°r|Ny√≠regyh√°za|Eger|Veszpr√©m|Szombathely|Sopron|Tatab√°nya|P√°pa|√ârd|Buda√∂rs|Pest megye|B√°cs-Kiskun megye|Fej√©r megye|Gy≈ër-Moson-Sopron megye)", html, flags=re.IGNORECASE)
        if mloc:
            location = mloc.group(1).strip()

    # ut√≥form√°z√°s
    if isinstance(location, str):
        location = location.replace("_", " ").replace("megye", "megye").strip(", ")

    return company, location

def fetch_job_meta(url: str, session: requests.Session = None, retries: int = 2, pause: float = 0.25):
    sess = session or requests.Session()
    last_err = None
    for _ in range(retries + 1):
        try:
            r = sess.get(url, headers=HEADERS, timeout=25)
            r.raise_for_status()
            r.encoding = "utf-8"
            company, location = extract_company_location_from_html(r.text)
            return (company, location)
        except Exception as e:
            last_err = e
            time.sleep(pause)
    # ha nem siker√ºlt, t√©rj√ºnk vissza None-okkal
    return (None, None)

def parse_company_from_summary(summary: str):
    if not isinstance(summary, str):
        return None
    m = re.search(r"Hirdet≈ë\s*c√©g\s*:\s*([^,|;]+)", summary, flags=re.IGNORECASE)
    return m.group(1).strip() if m else None

# Scraper futtat√°sa h√°tt√©rben
def run_scraper_async(selected_categories, progress_queue):
    all_rows = []
    seen_links = set()
    
    # Feed lista gener√°l√°sa a kiv√°lasztott kateg√≥ri√°k alapj√°n
    feed_list = [
        ("Profession ‚Äì IT f≈ëfeed", "https://www.profession.hu/partner/files/rss-it.rss"),
        ("Profession ‚Äì Fejleszt≈ë", "https://www.profession.hu/allasok/1,0,0,fejleszt≈ë?rss"),
        ("Profession ‚Äì Programoz√≥", "https://www.profession.hu/allasok/1,0,0,programoz√≥?rss"),
        ("Profession ‚Äì Szoftver", "https://www.profession.hu/allasok/1,0,0,szoftver?rss")
    ]
    
    # Csak a legfontosabb kulcsszavakhoz gener√°lunk feedet (max 3 per kateg√≥ria)
    for cat_id in selected_categories:
        if cat_id in CATEGORIES:
            keywords = CATEGORIES[cat_id]["keywords"][:3]  # Csak az els≈ë 3 kulcssz√≥
            for keyword in keywords:
                feed_list.append((f"Profession ‚Äì {keyword}", build_feed_url(keyword)))
    
    sess = requests.Session()
    total_feeds = len(feed_list)
    
    for i, (name, url) in enumerate(feed_list):
        progress_queue.put({"progress": (i / total_feeds) * 100, "status": f"Feldolgoz√°s: {name}"})
        
        try:
            items = fetch_rss_items(name, url)
            progress_queue.put({"debug": f"{name}: {len(items)} √°ll√°s tal√°lva"})
            print(f"DEBUG: {name} - {len(items)} √°ll√°s")
        except Exception as e:
            progress_queue.put({"error": f"Kihagyva ({name}): {str(e)}"})
            print(f"ERROR: {name} - {str(e)}")
            continue

        for it in items:
            link = it["Link"]
            if not link or link in seen_links:
                continue

            title = it["Poz√≠ci√≥"]
            desc = it["Le√≠r√°s"]
            if not is_probably_dev(title, desc):
                continue

            company, location = fetch_job_meta(link, session=sess, retries=1, pause=0.2)
            if not company:
                company = parse_company_from_summary(desc)

            seen_links.add(link)
            all_rows.append({
                "id": len(all_rows) + 1,
                "forras": it["Forr√°s"],
                "pozicio": title,
                "ceg": company or "N/A",
                "lokacio": location or "N/A",
                "link": link,
                "publikalva": it["Publik√°lva"],
                "lekeres_datuma": datetime.today().strftime("%Y-%m-%d"),
                "leiras": (desc[:200] if isinstance(desc, str) else "")
            })

            time.sleep(0.1)

        time.sleep(0.1)
    
    # Glob√°lis v√°ltoz√≥ friss√≠t√©se
    global scraped_jobs
    scraped_jobs = all_rows
    
    progress_queue.put({
        "progress": 100, 
        "status": "K√©sz!", 
        "data": all_rows,
        "stats": {
            "total_feeds": total_feeds,
            "total_jobs": len(all_rows),
            "unique_links": len(seen_links)
        }
    })

# API v√©gpontok
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/portals')
def get_portals():
    return jsonify(PORTALS)

@app.route('/api/categories')
def get_categories():
    return jsonify(CATEGORIES)

@app.route('/api/search', methods=['POST'])
def search_jobs():
    data = request.json
    selected_categories = data.get('categories', [])
    
    if not selected_categories:
        return jsonify({"error": "V√°lassz legal√°bb egy kateg√≥ri√°t!"}), 400
    
    try:
        # Scraper futtat√°sa szinkron m√≥don (egyszer≈±s√≠tett verzi√≥)
        all_rows = []
        seen_links = set()
        
        # Turb√≥ kulcsszavas keres√©s - minden kulcssz√≥hoz k√ºl√∂n keres√©s
        search_queries = []
        
        # Alap IT f≈ëfeed
        search_queries.append(("Profession ‚Äì IT f≈ëfeed", "https://www.profession.hu/partner/files/rss-it.rss"))
        
        # HTML scraping - csak a legfontosabb kulcsszavak
        priority_keywords = [
            # Legfontosabb ter√ºletek (kevesebb duplik√°ci√≥)
            "fejleszt≈ë", "programoz√≥", "szoftver", "szoftverm√©rn√∂k", "rendszerm√©rn√∂k",
            "frontend", "backend", "full stack", "devops", "data scientist", "mobile"
        ]
        
        for keyword in priority_keywords:
            if keyword in ALL_KEYWORDS:
                # HTML scraping URL (nem RSS)
                search_queries.append((f"Profession ‚Äì {keyword}", f"https://www.profession.hu/allasok/1,0,0,{quote(keyword, safe='')}"))
        
        # Alternat√≠v megk√∂zel√≠t√©s: teljesen k√ºl√∂nb√∂z≈ë keres√©sek
        alternative_searches = [
            # K√ºl√∂nb√∂z≈ë poz√≠ci√≥k
            ("Profession ‚Äì IT Manager", "https://www.profession.hu/allasok/1,0,0,it%20manager"),
            ("Profession ‚Äì System Admin", "https://www.profession.hu/allasok/1,0,0,rendszergazda"),
            ("Profession ‚Äì Database", "https://www.profession.hu/allasok/1,0,0,adatb√°zis"),
            ("Profession ‚Äì Network", "https://www.profession.hu/allasok/1,0,0,h√°l√≥zat"),
            ("Profession ‚Äì Security", "https://www.profession.hu/allasok/1,0,0,biztons√°g"),
            # K√ºl√∂nb√∂z≈ë technol√≥gi√°k
            ("Profession ‚Äì Docker", "https://www.profession.hu/allasok/1,0,0,docker"),
            ("Profession ‚Äì Kubernetes", "https://www.profession.hu/allasok/1,0,0,kubernetes"),
            ("Profession ‚Äì AWS", "https://www.profession.hu/allasok/1,0,0,aws"),
            ("Profession ‚Äì Azure", "https://www.profession.hu/allasok/1,0,0,azure"),
            ("Profession ‚Äì SQL", "https://www.profession.hu/allasok/1,0,0,sql")
        ]
        
        for name, url in alternative_searches:
            search_queries.append((name, url))
        
        print(f"üîç √ñsszesen {len(search_queries)} kulcsszavas keres√©s + IT f≈ëfeed")
        print(f"üìù Kulcsszavak: {len(ALL_KEYWORDS)} egyedi kulcssz√≥")
        
        sess = requests.Session()
        
        per_source_kept = defaultdict(int)
        per_source_skipped = defaultdict(int)
        
        total_queries = len(search_queries)
        for i, (name, keyword_or_url) in enumerate(search_queries):
            try:
                # Progress tracking
                progress = (i / total_queries) * 100
                print(f"üìä Progress: {progress:.1f}% - {name}")
                
                # URL meghat√°roz√°sa
                if keyword_or_url.startswith("http"):
                    if keyword_or_url.endswith(".rss"):
                        # IT f≈ëfeed - RSS
                        url = keyword_or_url
                        items = fetch_rss_items(name, url)
                    else:
                        # HTML scraping
                        url = keyword_or_url
                        items = fetch_html_jobs(name, url)
                else:
                    # Kulcsszavas keres√©s - HTML scraping
                    url = f"https://www.profession.hu/allasok/1,0,0,{quote(keyword_or_url, safe='')}"
                    items = fetch_html_jobs(name, url)
                print(f"üîé {name} - {len(items)} √°ll√°s")
                
                # Debug: els≈ë n√©h√°ny link ellen≈ërz√©se
                if items:
                    sample_links = [item["Link"] for item in items[:3]]
                    print(f"   Sample links: {sample_links}")
                else:
                    print(f"   ‚ö†Ô∏è Nincs √°ll√°s ebben a feed-ben: {url}")
                
                kept = 0
                skipped = 0
                
                for it in items:
                    link = it["Link"]
                    if not link:
                        skipped += 1
                        continue

                    # Csak a teljes link alapj√°n duplik√°ci√≥ ellen≈ërz√©s (nem a session param√©terek miatt)
                    clean_link = link.split('?')[0]  # Elt√°vol√≠tjuk a query param√©tereket
                    if clean_link in seen_links:
                        skipped += 1
                        continue

                    title = it["Poz√≠ci√≥"]
                    desc = it["Le√≠r√°s"]
                    if not is_probably_dev(title, desc):
                        skipped += 1
                        continue

                    # HTML scraping-b≈ël m√°r van c√©g √©s lok√°ci√≥
                    company = it.get("C√©g", "") or parse_company_from_summary(desc) or "N/A"
                    location = it.get("Lok√°ci√≥", "") or "N/A"

                    seen_links.add(clean_link)
                    all_rows.append({
                        "id": len(all_rows) + 1,
                        "forras": it["Forr√°s"],
                        "pozicio": title,
                        "ceg": company or "N/A",
                        "lokacio": location or "N/A",
                        "link": link,  # Eredeti linket t√°roljuk
                        "publikalva": it["Publik√°lva"],
                        "lekeres_datuma": datetime.today().strftime("%Y-%m-%d"),
                        "leiras": (desc[:200] if isinstance(desc, str) else "")
                    })
                    kept += 1

                    # Gyors m√≥d: nincs delay (tesztel√©shez)
                    # time.sleep(0.15)

                per_source_kept[name] = kept
                per_source_skipped[name] = skipped
                
                print(f"   ‚úÖ Megtartva: {kept}, Kihagyva: {skipped}")
                
                # Debug: duplik√°ci√≥ statisztik√°k
                if kept > 0:
                    print(f"   üìä Duplik√°ci√≥k: {skipped} (√∂sszesen {len(seen_links)} egyedi link)")
                    if skipped > kept:
                        print(f"   ‚ö†Ô∏è Sok duplik√°ci√≥ - val√≥sz√≠n≈±leg ugyanazok az √°ll√°sok k√ºl√∂nb√∂z≈ë kulcsszavakkal")
                
                # K√≠m√©let a szerver fel√© (feedek k√∂z√∂tt)
                time.sleep(0.15)

            except Exception as e:
                print(f"‚ö†Ô∏è Kihagyva ({name}): {str(e)}")
                continue
        
        # Glob√°lis v√°ltoz√≥ friss√≠t√©se
        global scraped_jobs
        scraped_jobs = all_rows
        
        print(f"\n‚úÖ {len(all_rows)} fejleszt≈ëi √°ll√°s tal√°lva, {len(search_queries)} kulcssz√≥val")
        
        # Top forr√°sok statisztik√°ja
        print("\nüìä Forr√°s√∂sszegz√©s (megtartott / kihagyott):")
        sorted_sources = sorted(per_source_kept.items(), key=lambda x: x[1], reverse=True)[:10]
        for name, kept in sorted_sources:
            print(f"  ‚Ä¢ {name}: {kept} / {per_source_skipped[name]}")
        
        return jsonify({
            "message": "Turb√≥ keres√©s befejezve", 
            "total_jobs": len(all_rows),
            "total_searches": len(search_queries),
            "unique_links": len(seen_links),
            "top_sources": dict(sorted_sources),
            "jobs": all_rows[:20]  # Els≈ë 20 √°ll√°s
        })
        
    except Exception as e:
        return jsonify({"error": f"Hiba a keres√©s sor√°n: {str(e)}"}), 500

@app.route('/api/progress')
def get_progress():
    # Egyszer≈±s√≠tett progress - val√≥s implement√°ci√≥ban session/task ID kellene
    return jsonify({"progress": 0, "status": "Keres√©s..."})

# Glob√°lis v√°ltoz√≥ a scraped adatok t√°rol√°s√°hoz
scraped_jobs = []

@app.route('/api/jobs')
def get_jobs():
    # Visszaadjuk a scraped √°ll√°sokat
    return jsonify(scraped_jobs)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=False, host='0.0.0.0', port=port)
